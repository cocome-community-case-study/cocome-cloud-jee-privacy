%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3, 2016-12-29

\chapter{Evaluation}
\label{ch:Evaluation}

This chapter is structured as follows: Initially the concept is elaborated (\autoref{sec:Evaluation:concept}), followed by evaluation scenarios (\autoref{sec:Evaluation:scenarios}) and the evaluation of the single tasks: monitoring (\autoref{sec:Evaluation:monitoring}), privacy analysis (\autoref{sec:Evaluation:monitoring}), model generation (\autoref{sec:Evaluation:privacyanalysis}), adaptation planning (\autoref{sec:Evaluation:planning}) and finally adaptation execution (\autoref{sec:Evaluation:execution}). Note, all evaluation models, test data and results can be found at \cite{privacy.PW}.

\section{Evaluation Design}
\label{sec:Evaluation:concept}

iObserve Privacy is a complex approach with many depending tasks. Evaluating the program as a whole is next to impossible due to the multiplexing dependencies. The evaluation factors would not be manageable and inconclusive results would make the evaluation itself pointless. So we decided to evaluate every task independently. The order and structure got inspired by the iObserve pipeline.

%Accuracy => Scenarios => unbiased
The task evaluation is generally split into an \textit{Accuracy} evaluation and a \textit{Scalability} evaluation. The accuracy evaluation aims for the correct functionality, testing whether the actual results are conform to expected results. For the evaluation we are creating a set of \textit{Evaluation Scenarios}, which reflect real world situations by defining a staring point and an expected endpoint. If the systems result differs from the endpoint, the reasons must be found and analysed.

%Scalability => Runtime behaviour, automatic generated PCMs, 1 > 10 > 100 > 1000 ... logarithmic scale
The scalability evaluation aims for the systems runtime characteristic, based on an increasing work load. The actual accuracy result of the task doesn't interest during this analysis. The primary measurement is the task response time, dependent on the assembly context count and resource container count. Both axis are logarithmic scaled, so the response behaviour is clearly visible. The individual models are randomly generated, based on a repository model input.

We will use the \textit{Jaccard Coefficient} to evaluate model changes during the accuracy evaluation. Prior to the execution a target model is created, representing the desired post-execution state. The runtime model is compared to the target model, differences get calculated, as well as the jaccard coefficient. The coefficient is defined as the \textit{intersection set} of runtime and target model \textit{divided} through the \textit{union set} of these models. If the models are completely equal, the result is \textit{1.0} \cite{Andale.20161202}. We are comparing the system model, the resource environment model and the allocation model. The repository and usage model model is not modified by iObserve Privacy and therefore don't need any comparison. All models elements are matched by element content. The system and resource environment model are also compared by element ID. All models modified by iObserve Privacy are order independent, so an order match, like the \textit{Spearman Coefficient}, is not required.


\section{Evaluation Scenarios}
\label{sec:Evaluation:scenarios}

The scenarios are structured in \textit{PRE}, \textit{EVENT}, \textit{REACTION} and \textit{POST}. Pre describes the distributed software system before the event takes place. The event is a trigger for a certain process or task chain, usually referenced as reaction. Post defines the state of the software system after the reaction.

The scenarios describe the behaviour of iObserve Privacy through all tasks, while each task gets evaluated individually. Nevertheless, details of an scenario may need clarification during the evaluation of this task.

The scenarios are derived from \cite{Heinrich.2016b}. This paper describes potential runtime changes to a distributed software system. However, not all mentioned scenarios are of interest in the privacy analysis context. Scenarios 1 and 2 represent the observed system runtime changes. However, major iOberve privacy failing scenarios are not covered. This is due to the iObserve Privacy design specific nature of error states. Scenario 3 and 4 cover these error.

\subsection{Scenario 1: Default}
\label{eval:scenario:1}
% PRE: Amazon Deployment @ EU WEST
% CASE: Critical Error => Amazon moves instances to EU WEST & US EAST
% POST: Migration Monitored, Privacy Analysis started
%					=> IF OK:	Do nothing 
%					=> IF BAD:	Calculate Alternative (Privacy Compliant) => Plan => Adapt => Evaluate!
This scenario describes the "default" setting. A migration gets monitored, analysed, an alternative deployment calculated and finally migrated. The process works completely automated. This means no operator interaction is required and no task error is generated. As trigger for privacy analysis the server geo-location migration is used.
\begin{itemize}
	\setlength\itemsep{0em}
	\item \textbf{PRE}: All components of the software system are deployed on Amazons EC2 service on the \textit{EU Frankfurt} location. The system is privacy compliant.
	\item \textbf{EVENT}: Amazons EU Frankfurt data centre has a critical failure. As a result Amazon starts migrating local virtual machines towards the US Ohio and EU Ireland locations.
	\item \textbf{REACTION}: iObserve Privacy monitors the migration and starts a privacy analysis. If the analysis doesn't show a privacy violation no further action is taken. If the analysis shows a privacy violation, an alternative, privacy compliant deployment gets computed, a system adaptation plan gets calculated and finally executed.
	\item \textbf{POST}: The software system is in a privacy compliant state.
\end{itemize}


\subsection{Scenario 2: System extension}
\label{eval:scenario:2}
% PRE: 	Privacy components @ Microsoft; Non-Privacy @ UKRAINE
% CASE:	New DePersonalized Component gets deployed @ UKRAINE
% POST: Deployment Monitored => Privacy Analysis started => "Joining Data Streams" found => Calculate Alternative (Privacy Compliant) => Plan => Adapt => Evaluate!

This scenario describes the deployment runtime change. The deployment of a new software component triggers a privacy analysis, which detects "joining data streams" (see \autoref{sec:PrivacyAnalysis:theory}). This triggers the generation of an alternative deployment and the system adaptation. The pipeline works like in Scenario 1 without any operator interaction.
\begin{itemize}
	\setlength\itemsep{0em}
	\item \textbf{PRE}: All personal categorized components of the software system are deployed on Amazons EC2 service on the \textit{EU Frankfurt} location. All other components are hosted by an Ukrainian provider. The system is privacy compliant.
	\item \textbf{EVENT}: The system operator adds another as depersonalised categorized component to the Ukrainian host.
	\item \textbf{REACTION}: iObserve Privacy monitors the migration and starts a privacy analysis. The privacy analysis shows a privacy violation due to joining data streams. An alternative, privacy compliant deployment gets computed, a system adaptation plan gets calculated and finally executed.
	\item \textbf{POST}: The software system is in a privacy compliant state.
\end{itemize}

\subsection{Scenario 3: Failing Adaptation}
\label{eval:scenario:3}
% PRE: 	All @ Cheap Hoster (Europe SLA)
% CASE:	Some components migrate @ UKRAINE
% POST: Deployment Monitored => Privacy Analysis started => Privacy Violation found => Calculate Alternative (Privacy Compliant) => Plan => Adapt can't be done automatically => Call the Operator
This scenario describes the operator-in-the-loop use case, when an adaptation sequence can't be executed automatically. The migration of a software component results in a privacy violating state. After the generation of a privacy compliant alternative and the calculation of the adaptation sequence, the operator is required. One or more adaptation actions can't be executed automatically. This is usually due to missing source control, like the \textit{Change Repository Component Action} or the \textit{Allocation Action}.
\begin{itemize}
	\setlength\itemsep{0em}
	\item \textbf{PRE}: All components of the software system are hosted multiple server instances by cloud reseller. 
	\item \textbf{EVENT}: The reseller starts migrating his server to another cloud providers.
	\item \textbf{REACTION}: iObserve Privacy monitors the migration and starts a privacy analysis. The privacy analysis shows a privacy violation. An alternative, privacy compliant deployment gets computed. The adaptation calculation shows certain steps can't be executed automatically. After ordering the adaptation steps the operator gets informed for manual execution.
	\item \textbf{POST}: iObserve Privacy shows the operator the adaptation sequence with emphasis on the manual tasks.
\end{itemize}


\subsection{Scenario 4: Missing Alternative}
\label{eval:scenario:4}
% PRE: 	All @ Cheap Hoster (Europe SLA)
% CASE:	Some components migrate @ UKRAINE
% POST: Deployment Monitored => Privacy Analysis started => Privacy Violation found => Calculate Alternative (Privacy Compliant) => No Privacy Compliant Alternative Found => Call Operator
This scenario describes the use case, where no privacy compliant, alternative deployment could be calculated. The migration of a software components results in a privacy violating state. The calculation of privacy compliant alternatives deployment fails. The operator needs to be informed about the current situation.
\begin{itemize}
	\setlength\itemsep{0em}
	\item \textbf{PRE}: All components of the software system are hosted multiple server instances by cloud reseller. 
	\item \textbf{EVENT}: The reseller starts migrating his server to another cloud providers.
	\item \textbf{REACTION}: iObserve Privacy monitors the migration and starts a privacy analysis. The privacy analysis shows a privacy violation. An alternative, privacy compliant deployment can't be computed.
	\item \textbf{POST}: iObserve Privacy notifies the operator about the missing privacy compliant re-deployment model.
\end{itemize}

%\subsection{Scenario 5}
% PRE: 	Privacy components @ Amazone EU WEST; Non-Privacy @ Cheap Hoster (Europe SLA)
% CASE:	Cheap Hoster constantly migrates to the cheapest hoster => Moves two Depersonal instances to the same server in Ukraine
% POST: Migration Monitored, Privacy Analysis started
%			=> "Joining Data Streams" found => Calculate Alternative (Privacy Compliant) => Plan => Adapt => Evaluate!

\subsection{Futile Scenario}
There are a couple of scenarios which don't apply to iObserve Privacy, due to various reasons \cite{Heinrich.2016b}. We will elaborate those scenarios shortly.

%Performance
Performance or workload characteristics are not tackled, since performance and privacy analysis combined wouldn't be manageable in the scope of this thesis.

%Undeployment
The un-deployment or de-replication are two scenarios which reduce the complexity of the privacy analysis. A privacy violation can't be triggered by eliminating a component and/or a server from the system.

%Replication
The replication of a server, with all its components, will trigger a deployment event. This means, this scenario is already covered by \textit{Scenario 2}.


\section{Evaluation Models}
\label{sec:Evaluation:models}

In the previous sections we defined a couple of scenarios for the evaluation. In order to execute these scenarios, we need PCM Privacy models (\autoref{ch:pcmExtension}). Scenario and model need to get selected individually, depending on task to evaluate.

\subsection{CoCOME-Cloud}
\label{sec:eval:models:cocome}

The \textit{CoCOME Cloud} PCM model is a representation of the CoCOME system as a distributed cloud variant. It is a representation of a supermarket IT infrastructure. It exists of six individual deployed components:  \textit{logic.webservice.cashdeskline.cashdeskservice}, \textit{Cloud.Web}, \textit{traidingsystem.inventory}, \textit{traidingsystem.cashdeskline}, \textit{webservice.inventory} and \textit{traidingsystem.external.bank}. The system is oriented on real systems with dozens of interfaces and multiple composite components. As a result, CoCOME-Cloud is very complex and not suited for the evaluation of specific aspects. However, it is as the only available model fully specified and "PerOpteryx ready". See \cite{Heinrich.2015} for detailed information on CoCOME.


\subsection{Medi System}
\label{sec:eval:models:medSys}

\begin{figure}[h]
	\centering
	\includegraphics[trim = 0mm 10mm 0mm 20mm, clip, width=0.90\textwidth]{graphs/medSystem_noserver}
	\caption{Initial component categorization}
	\label{fig:model:medi}
\end{figure}

The \textit{Medi System} is an PCM model, specially developed for the evaluation of this thesis. It is supposed to reflect the web system of a medical insurance. The required and provided interfaces are reduced to the minimal necessary, to limit side effects and to gain meaningful results. \autoref{fig:model:medi} shows the medi system with all components and interface connections. The deployment will depend on the evaluation scenario.


\subsection{Generated Models}
\label{sec:Evaluation:models:generated}

We developed a model generator and model modificator for the scalability analysis. The generator requires an input repository and creates a valid PCM Privacy model with the given amount of assembly contexts and resource container. The contained component in the assembly context is randomly selected, as well as the resource container it is allocated on. All required interfaces are correctly connected, primarily to provided interfaces without an existing connection. The privacy categorization of an Assembly Connector Privacy is randomly chosen with a distribution of 15\% Personal, 35\% Depersonalised and 50\% Anonymized.

The model modificator adapts the system randomly, based on action counts specified. The modification supports server acquisition and termination and assembly context allocation, deallocation and migration. Further it supports the exchange of the contained repository component for a component with the same interfaces. Note, the generated models are only suited for the scalability analysis. 



\section{Transformation}
\label{sec:Evaluation:monitoring}

The Transformation evaluation aims for an accuracy and scalability test of the pipeline trigger events and the actual transformation of the send information onto the model. \textit{Scenario \#1} (\autoref{eval:scenario:1}) and \textit{Scenario \#2} (\autoref{eval:scenario:2}) describe the two possible triggers: the \textit{TDeployment Event}, when a component got deployed on a server, and the \textit{TGeoLocation Event}, when the geo-location of a server changes. 

\subsection{Transformation: Accuracy Evaluation}

For the accuracy evaluation we are using the CoCOME-Cloud model (\autoref{sec:eval:models:cocome}), since it is completely specified and reflects the a real system the best way available. We need to show, that the TDeployment, TUndeployment and TGeoLocation Event process transform the send data correctly onto the PCM model. Potential errors must be handled and avoided. For this purpose we will use two executions. First, we will execute a logically valid input set of events, to show the correctness of the transformation. In a second execution will show that logically wrong inputs are processed correctly. Both runs start with am empty allocation model.

\autoref{tab:valid_run} shows the initial input event sequence. Initially all components get deployed, followed by an geo-location update on all servers, followed by a re-deployment of the \textit{cloud.web} component from \textit{Server1-EU} to \textit{Server5-EU} and finally a geo-location update on \textit{Server4-EU} to Ukraine.

\begin{table}[h]
	\centering
	\begin{tabular}{r | l}
		\hline
		\textbf{Action} & \textbf{Values}\\
		\hline
		Deployment & tradingsystem.external.Bank on Server6-EU\\
		Deployment & tradingsystem.cashdeskline on Server4-EU\\
		Deployment & cloud.web on Server1-EU\\
		Deployment & webservice.inventory on Server1-EU\\
		Deployment & tradingsystem.inventory on Server2-EU\\
		Deployment & logic.webservice.cashdeskline.cashdeskservice on Server3-EU\\
		GeoLocation & Server1-EU on 276 (GER)\\
		GeoLocation & Server2-EU on 276 (GER)\\
		GeoLocation & Server3-EU on 250 (FRA)\\
		GeoLocation & Server4-EU on 250 (FRA)\\
		GeoLocation & Server5-EU on 826 (GBR)\\
		GeoLocation & Server6-EU on 826 (GBR)\\
		UnDeployment & cloud.web from Server1-EU\\
		Deployment & cloud.web on Server5-EU\\
		GeoLocation & Server4-EU on 804 (UKR)\\
		\hline
		\end{tabular}
	\caption{The correct execution set}
	\label{tab:valid_run}
\end{table}

We expect a run without any errors, an allocation model, which represents the described deployment and a design decisions model, which the according degree of freedoms.

The results are unbiased. The system reports no errors and the models represent the system exactly as intended. The \textit{Jaccard Coefficient} is \textit{1.0}.

\begin{table}[h]
	\centering
	\begin{tabular}{r | l}
		\hline
		\textbf{Action} & \textbf{Values}\\
		\hline
		\multicolumn{2}{ c }{\autoref{tab:valid_run} commands}\\
		Deployment & cloud.web on Server1-EU*\\
		UnDeployment & cloud.web from Server5-EU\\
		UnDeployment & cloud.web from Server5-EU*\\
		Deployment & cloud.web on Server7-EU*\\
		Deployment & IllegalComonent on Server1-EU*\\
		UnDeployment & IllegalComonent from Server1-EU*\\
		UnDeployment & tradingsystem.inventory from Server3-EU*\\
		GeoLocation & Server7-EU on 826 (GBR)*\\
		\hline
	\end{tabular}
	\caption{The error execution set}
	\label{tab:error_run}
\end{table}

To test the robustness, we need to input invalid, logically false, events. To gain a valid system state, we are starting with the valid order (\autoref{tab:valid_run}) and append illegal orders. \autoref{tab:error_run} shows the exact execution sequence. Illegal events are marked with a *. We expect these orders to give a warning and to be ignored. The system must continue running. The test includes the following cases: Deployment of an already deployed component, deployment or undeployed on a non-existing server, geo-location record from a non-existing server, un-deployment of a non-existing deployment.


The error run shows ends up to be exactly as intended. All faulty commands got ignored and the \textit{Jaccard Coefficient} is \textit{1.0}. The results show, that the problem and research question in \autoref{sec:Introduction:problems} was successfully solved.


\subsection{Transformation: Scalability Evaluation}

For the scalability analysis we are using the Medi-System model with generated input. The inputs are logically and syntax valid. 30\% of the inputs are TDeployment and TUnDeployment events, distributed relative to the current allocation status. The other 70\% of inputs are TGeolocation events, randomly distributed over all available servers. Every measurement was repeated ten times to eliminate potential measurement errors. The log outputs remain active, the snapshot creation is deactivated, so no further pipeline filters get activated. We use input sizes from 10 to one million on a logarithmic scale.

\begin{figure}[h]
	\centering
	\includegraphics[trim = 10mm 90mm 10mm 110mm, clip, width=0.90\textwidth]{graphs/Runtime_Transformation}
	\caption{Transformation runtime \& SD}
	\label{fig:eval:trans:runtime}
\end{figure}

The results (\autoref{fig:eval:trans:runtime}) show a linear runtime behaviour, for the maximal input size of 1 mio events the execution takes about 820 seconds. The according standard deviation of 22 makes it a stable and fast result.

\section{Privacy Analysis}
\label{sec:Evaluation:privacyanalysis}

The \textit{Privacy Analysis} was extensively discussed in \autoref{ch:PrivacyConcept} (Privacy Concept) and \autoref{ch:PrivacyAnalysis} (Privacy Analysis). As described there, the privacy analysis consists of two sequential parts: \textit{Component Classification} and \textit{Deployment Analysis}. According to this tasks, the accuracy evaluation is also split. The accuracy evaluation uses the Medi-System model (\autoref{sec:eval:models:medSys}), due to its moderate complexity level, where effects like the \textit{Joining Data Stream} occurs, but the results are still traceable.

\subsection{Privacy Analysis: Accuracy Evaluation}

We will show, that the component classification categorizes components correctly, by finding \textit{joining data streams} in inter component communication. Further, we will show, that the deployment analysis finds \textit{joining data streams} on the deployment level. As a result, we demonstrate the correctness of our privacy analysis, as specified in the goal section (\autoref{sec:Introduction:goals}).

The scenarios \#1 (\autoref{eval:scenario:1}) and \#2 (\autoref{eval:scenario:1}) aim to trigger a privacy analysis. We showed in \autoref{sec:Evaluation:monitoring}, that both trigger, the TDeployment Event and the TGeoLocation Event are correctly processed and the information transformed onto the PCM model. Both trigger start the same privacy analysis and are therefore equivalent. 

\begin{figure}[h]
	\centering
	\includegraphics[trim = 0mm 10mm 0mm 10mm, clip, width=0.75\textwidth]{graphs/medSys_eval_pa_init}
	\caption{Initial system state}
	\label{fig:eval:pa:init}
\end{figure}

The initial system state is as show in \autoref{fig:eval:pa:init}. The system is privacy compliant and only the \textit{GovStatistics} component is not allocated onto a server. iObserve will trigger the pipeline by receiving a GeoLocation event, which migrates the \textit{Server2} to Belarus. We expect the initial component categorization to be equal to the most personal interface level the component has. After the \textit{Categorization Analysis} the \textit{MarketingAnalysis} component should be classified as personal, due to its two personal communication partners. The components \textit{ErrorDetection}, \textit{TreatmentAnalysis} and \textit{CourseAnalysis} data privacy level should remain unchanged, since they share a single depersonalised interface as data sources. The deployment must remain legal.

As a second trigger, we deploy the GovStatistics component onto Server2. The component must be tagged depersonalised and the deployment analysis must report a \textit{joining data stream} on Server2.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.48\textwidth}		
		\includegraphics[trim = 20mm 10mm 40mm 10mm, clip, width=0.99\textwidth]{graphs/medSys_eval_pa_tagging_init}
		\caption{Initial categorization}
		\label{fig:eval:pa:base_tag}
	\end{minipage}
	\begin{minipage}[b]{0.48\textwidth}
		\includegraphics[trim = 20mm 10mm 40mm 10mm, clip, width=0.99\textwidth]{graphs/medSys_eval_pa_tagging_analysis}
		\caption{Categorization analysis result}
		\label{fig:eval:pa:categorized}
	\end{minipage}
\end{figure}

The result is \textit{unbiased}. The trigger is correctly processed due to the \textit{Jaccard Coefficient} of \textit{1.0}. The \autoref{fig:eval:pa:base_tag} shows the initial component categorization. \autoref{fig:eval:pa:categorized} shows the categorization analysis result. Both states fulfil the expectations. The deployment analysis reports a legal deployment.

The second trigger, the component deployment on \textit{Server2}, reports a privacy violation. The cause is a joining data stream on Server2. This is what we provoked and expected. The \textit{Jaccard Coefficient} for the trigger processing is \textit{1.0}, again.

\begin{figure}[h]
	\centering
	\includegraphics[trim = 0mm 10mm 0mm 10mm, clip, width=0.90\textwidth]{graphs/medSys_eval_pa_da}
	\caption{Deployment analysis result}
	\label{fig:eval:pa:depl_ana}
\end{figure}

We have shown, that the component classification algorithm and the deployment analysis works as intended, providing a privacy analysis on a architectural level (see \autoref{sec:Introduction:goals}). We didn't show every possible privacy violation, however showed the most difficult analysis work: The detection of joining data streams on component categorization and deployment analysis level. And the correct identification of a set of components, located on a server, sharing the same single depersonalised component as a data source. We will use further exemplary privacy violations in the other accuracy evaluations.

\subsection{Privacy Analysis: Scalability Evaluation}
\label{sec:Evaluation:privacyanalysis:scale}

For the scalability evaluation of the privacy analysis, we will use the generated model (\autoref{sec:Evaluation:models:generated}), since models of the intended scale are not constructable by hand.

The time measurement starts before the graph is constructed from the model. Time rounds are taken to measure the component classification and the deployment analysis. Complex outputs are deactivated to minimise random influences.

We will measure a graph size from 10 to one million nodes on a logarithmic scale, with a distribution of 40\% Resource Container and 60\% Assembly Contexts. The classification of the Assembly Connectors is distributed among 15\% Personal, 35\% Depersonalised and 50\% Anonymous. We expect this configuration to reflect a real world setting, with multiple components on a single server and only a view unused servers and an slightly increasing effort of to analyse the models. The \textit{legal} geo-locations consist of 40 random countries. Each model measurement is repeated ten times to minimize variation effects.

\begin{figure}[h]
	\centering
	\includegraphics[trim = 15mm 95mm 13mm 110mm, clip, width=0.95\textwidth]{graphs/Runtime_pa}
	\caption{Privacy Analysis runtime}
	\label{fig:eval:pa:runtime}
\end{figure}


\begin{figure}[h]
	\centering
	\includegraphics[trim = 5mm 30mm 10mm 30mm, clip, width=0.95\textwidth]{graphs/Runtime_pa_sd}
	\caption{Privacy Analysis runtime standard deviation}
	\label{fig:eval:pa:runtime_sd}
\end{figure}

\autoref{fig:eval:pa:runtime} shows the accumulated runtime in the order the tasks are executed. Initially the graph for the privacy analysis gets created, followed by the component classification and finally the deployment analysis. The graph shows a nearly linear increasing runtime. The major time is consumed by the graph construction, while the component classification and deployment analysis only show minor size effects. The whole runtime for one million nodes is still significantly below ten seconds. The limiting resource for further scalability test is the graph generation. However, graphs with the size of 1000 and more nodes are already very unlikely.

The standard deviation \autoref{fig:eval:pa:runtime_sd} is increasing - in general - linear with the model size. The deployment analysis however, shows some irregular behaviour. This could be due to the randomly selected save countries. A standard deviation of three times the actual mean runtime does means, significantly runtime computation differences must occur. Nevertheless, a maximum deployment analysis time of ten seconds on a one million nodes graph is still considered very fast. And an overall privacy analysis runtime of maximal 15 seconds is also very quick.


\section{Model Generation}
\label{sec:Evaluation:generation}

The privacy compliant model generation is only evaluated under the \textit{Accuracy} aspect. The \textit{Scalability} evaluation was already performed by the creators of PerOpteryx (see \cite{Koziolek.2014}) and our Privacy Analysis was extensively evaluated in \autoref{sec:Evaluation:privacyanalysis}. We will focus on the in \autoref{sec:Introduction:problems} specified problem of generating a privacy compliant redeployment model.

For this evaluation we will use the CoCOME model (\autoref{sec:eval:models:cocome}) together with the scenarios \#2 (\autoref{eval:scenario:2}) and \#4 (\autoref{eval:scenario:4}). Scenario \#2 describes a complete iObserve Privacy execution, including the successful execution of PerOpteryx, the model generation. Scenario \#4 describes the case, when PerOpteryx fails to generate a privacy compliant candidate. We will start with Scenario \#2.

We deploy the CoCOME components onto one EU server each and adjusted the individual server costs to reflect EU and Non-EU status. We will trigger the pipeline by moving the \textit{Server4-EU}, which hosts the as personal categorized component \textit{webservice.inventory}, to Ukraine.

After the execution of our model generation framework, \textit{PerOpteryx}, we expect the re-deployment model to be privacy compliant. This is the major concern and primary focus. However, we further anticipate a deployment with less server in use, as well as a migration of multiple \textit{depersonalised} components onto Non-EU servers.

PerOpteryx is configured to execute \textit{four iterations} with \textit{20 candidates} per iteration. This means, a total of 80 candidates will be created and evaluated. 


\begin{table}[h]
	\centering
	\begin{tabular}{ r | l | l | l }
		\hline
		\textbf{Component} & \textbf{Categorization} & \textbf{Deployment} & \textbf{Redeployment} \\
		\hline
		cloud.web & Depersonalized & Server1-EU & Server6-EU \\
		tradingsystem.inventory & Personal & Server2-EU & Server1-EU \\
		cashdeskline.cashdeskservice & Depersonalized & Server3-EU & Server2-NonEU \\
		webservice.inventory & Personal & Server4-EU & Server6-EU \\
		tradingsystem.cashdeskline & Depersonalized & Server5-EU & Server4-EU \\
		tradingsystem.external.Bank & Depersonalized & Server6-EU & Server3-NonEU \\
		\hline
	\end{tabular}
	\caption{Component categorization, runtime deployment and re-deployment}
	\label{tab:eval:gen:execution}
\end{table}

\autoref{tab:eval:gen:execution} shows the component, their data privacy level classification, the initial deployment and the generated re-deployment. An re-deployment model privacy analysis shows, that the model is privacy compliant. The major goal (see \autoref{sec:Introduction:goals}) got accomplished. Further, depersonalised components got moved to more cheap Non-EU severs and two components got deployed onto the same server. Both deployment goals indicate, that the \textit{most cost efficient} model got chosen - as intended. However, no component stays at this original server, which produces (unnecessary) migration costs. The execution time of PerOpteryx was about 30 seconds and elven of 80 candidates were privacy compliant.

In scenario \#4, no privacy compliant re-deployment model was found, PerOpteryx crashes or the any kind of error occurs. However, the end result stays unchanged, no valid re-deployment model is available after the PerOpteryx execution. We will provoke this situation by deploying all components onto a single server and moving this server to a Non-EU geo-location. We expect iObserve Privacy to report, that no privacy compliant PCM model was found. Basically, we expect, that iObserve invokes the \textit{operator-in-the-loop} for manual error treatment.

The execution shows, an error is written to the console, that the given re-deployment model is not privacy compliant. If a listener would be registered, the listener would be notified. The execution lives up to our expectations. 

\section{Adaptation Planning}
\label{sec:Evaluation:planning}

The Adaptation Planing, described in \autoref{ch:SysAdap}, aims for calculating an sequence of adaptation actions. The execution of this adaptation order must result in an runtime model, that is equivalent to the redeployment model. This is one of the research goals stated in \autoref{sec:Introduction:goals}. We will evaluate this task towards its accuracy and scalability aspects.

 
\subsection{Adaptation: Accuracy Evaluation}

We will continue scenario \#2 from generation evaluation (\autoref{sec:Evaluation:generation}) to show the continuous flow and re-establishment of the privacy compliance. To show, that the given adaptation order leads to a privacy compliant system, we will simulate the adaptation. This will be achieved by translating the adaptation order to an equivalent iObserve privacy input. After iObserve privacy computed the input, the \textit{Jaccard Coefficient} - between the computed re-deployment model from PerOpteryx and the iObserve runtime model - must be \textit{1.0}.

We expect the adaptation sequence to initially \textit{acquire} the newly used servers, followed by a series of \textit{migration} actions, which moves the components to their new servers. Finally there should be three \textit{terminate} actions to release the servers, that are no longer needed.

\begin{table}[h]
	\centering
	\begin{tabular}{r | l  l l l}
		\hline
		\textbf{Action} & \multicolumn{3}{ l }{\textbf{Values}}\\
		\hline
		Acquire & \multicolumn{3}{ l }{Server3-NonEU} \\
		Acquire & \multicolumn{3}{ l }{Server2-NonEU} \\
		Migrate & webservice.inventory & Server4-EU & -> & Server6-EU \\
		Migrate & tradingsystem.cashdeskline & Server5-EU & -> & Server4-EU \\
		Migrate & tradingsystem.external.Bank & Server6-EU & -> & Server3-NonEU \\
		Migrate & cashdeskline.cashdeskservice & Server3-EU & -> & Server2-NonEU \\
		Migrate & cloud.web & Server1-EU & -> & Server6-EU \\
		Migrate & tradingsystem.inventory & Server2-EU & -> & Server1-EU \\
		Terminate & \multicolumn{3}{ l }{Server3-EU} \\
		Terminate & \multicolumn{3}{ l }{Server2-EU} \\
		Terminate & \multicolumn{3}{ l }{Server5-EU} \\
		\hline
	\end{tabular}
	\caption{The ordered adaptation sequence}
	\label{tab:eval:adapt:action_order}
\end{table}

\autoref{tab:eval:adapt:action_order} shows the output adaptation sequence for scenario \#2. The result is unbiased. The in \autoref{tab:SysAdap:order} stated universal order is met. For the transformation onto iObersve input events, we can ignore the acquire actions, since the resource container already exist in the resource environment model. Every migrate action needs to be translated into two events: a TUndeployment event and a TDeployment event. The terminate can also be ignored, since there is no real server to release/terminate. We expect the system to be in a privacy compliant state after the execution of the iOberve input events. \autoref{tab:eval:adapt:action_input_result} shows the resulting iObserve input.

\begin{table}[h]
	\centering
	\begin{tabular}{r | l}
		\hline
		\textbf{Action} & \textbf{Values}\\
		\hline
		UnDeployment & webservice.inventory from Server4-EU\\
		Deployment & webservice.inventory on Server6-EU\\
		
		UnDeployment & tradingsystem.cashdeskline from Server5-EU\\
		Deployment & tradingsystem.cashdeskline on Server4-EU\\
		
		UnDeployment & tradingsystem.external.Bank from Server6-EU\\
		Deployment & tradingsystem.external.Bank on Server3-NonEU\\
		
		UnDeployment & cashdeskline.cashdeskservice from Server3-EU\\
		Deployment & cashdeskline.cashdeskservice on Server2-NonEU\\
		
		UnDeployment & cloud.web from Server1-EU\\
		Deployment & cloud.web on Server3-EU\\
		
		UnDeployment & tradingsystem.inventory from Server2-EU\\
		Deployment & tradingsystem.inventory on Server1-EU\\
		\hline
	\end{tabular}
	\caption{iObserve input event translated adaptation sequence}
	\label{tab:eval:adapt:action_input_result}
\end{table}


The execution of the transformed adaptation sequence (\autoref{tab:eval:adapt:action_input_result}) results in a privacy compliant PCM model, the post-execution runtime model and calculated redeployment model result in an Jaccard Coefficient of \textit{1.0}. Frankly speaking, the result is unbiased and solves the in \autoref{sec:Introduction:problems} specified problem of \textit{automated privacy compliance re-establishment}.

\begin{figure}[h]
	\centering
	\includegraphics[trim = 5mm 10mm 10mm 10mm, clip, width=0.65\textwidth]{graphs/medSystem_adap_calc_all_runtime}
	\caption{Runtime model}
	\label{fig:eval:adapt:4:run}
\end{figure}


We will continue to evaluate the \textit{System Adaptation} with scenario \#3 to show the limitations and so far unseen capabilities of the adaptation calculation. For this scenario the Medi-Sys will be used.

PerOpteryx only adapts the system given according to the \textit{design decision} model, it will only produce acquire, migrate and terminate actions - at least in our case. These are (usually) automatically executable. However, there are more potential actions specified, which can't be executed automatically. We will modify the Medi-Sys model to provoke one of each action. \autoref{fig:eval:adapt:4:run} shows the runtime model, \autoref{fig:eval:adapt:4:redepl} shows the re-deployment model. The anticipated difference between these models is shown in \autoref{tab:eval:adapt:4:action_provoke}. We expect each of the seven actions to be detected, except the \textit{replicate} action, since this is not (yet) supported by the adaptation calculation. It will be subsided by an \textit{aquire Server2-2} and an \textit{allocate GovStatistics on Server2-2}.

\begin{figure}[h]
	\centering
	\includegraphics[trim = 5mm 5mm 10mm 10mm, clip, width=0.65\textwidth]{graphs/medSystem_adap_calc_all_redepl}
	\caption{Redeployment model}
	\label{fig:eval:adapt:4:redepl}
\end{figure}

\begin{table}[h]
	\centering
	\begin{tabular}{r | l }
		\hline
		\textbf{Action} & \textbf{Values}\\
		\hline
		Acquire & Server-6\\
		Exchange Component & CourseAnalysis to CourseAnalysis-2\\
		Deallocate & MarketingAnalysis from Server-4\\
		Allocate & AccountingService-2 on Server-6\\
		Migrate & ErrorDetection from Server-3 to Server-6\\
		Replicate & Server-2 (with GovStatistics)\\
		Terminate & Server-4\\
		\hline
	\end{tabular}
	\caption{Expected adaptation sequence for Scenario \#4}
	\label{tab:eval:adapt:4:action_provoke}
\end{table}

The execution of the adaptation calculation shows the expected result (see \autoref{tab:eval:adapt:4:action_result}). The replicate action is split into the expected \textit{acquire} and \textit{allocate} actions (marked with an *).

\begin{table}[h]
	\centering
	\begin{tabular}{r | l }
		\hline
		\textbf{Action} & \textbf{Values}\\
		\hline
		Acquire & Server-6\\
		Acquire & Server2-2 (*)\\
		Exchange Component & CourseAnalysis to CourseAnalysis-2\\
		Allocate & AccountingService-2 on Server-6 (*)\\
		Allocate & GovStatistics-2 on Server2-2\\
		Deallocate & MarketingAnalysis from Server-4\\
		Migrate & ErrorDetection from Server-3 to Server-6\\
		Terminate & Server-4\\
		\hline
	\end{tabular}
	\caption{Expected adaptation sequence for Scenario \#4}
	\label{tab:eval:adapt:4:action_result}
\end{table}

\todo{what is not automatically executable?}


\subsection{Adaptation: Scalability Evaluation}

For the scalability analysis we are generating logically valid PCM Privacy models, modifying these and then running the \textit{Adaptation Calculation} and \textit{Adaptation Planning}. We will measure the runtimes of the graph creation, the adaptation calculation and the adaptation planning to gain a detailed runtime behaviour for each task.

The initial model is constructed in a way, that a model of size 100 consists of 60 assembly contexts and 40 resource containers. The modification is linear distributed over the acquire, terminate, allocate, deallocate, migrate and exchange component related modification. However, not every modification in the model leads must lead to an adaptation action. The adding of another resource container is only recognized as an acquire action, if an assembly context gets allocated on that resource container. The termination of a resource container, that hosts one or more components, leads also to a migration action. With this in mind, we measure the time of the adaptation calculation and adaptation planning based on the amount of modifications in the model. The model is three times as big as the modifications made. Each measurement was repeated ten times, the input sizes were 10, 100, 1000 and 10000. An evaluation run with 100000 provoked changes was aborted after twelve hours of computation.

\begin{figure}[h]
	\centering
	\includegraphics[trim = 15mm 95mm 15mm 105mm, clip, width=0.90\textwidth]{graphs/Runtime_adapt}
	\caption{Adaptation runtime}
	\label{fig:eval:adap:runtime}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[trim = 10mm 30mm 10mm 20mm, clip, width=0.90\textwidth]{graphs/Runtime_adapt_sd}
	\caption{Adaptation runtime SD}
	\label{fig:eval:adap:runtime_sd}
\end{figure}

\autoref{fig:eval:adap:runtime} shows the runtime results. The graph creation shows a linear runtime behaviour, like in \autoref{sec:Evaluation:privacyanalysis:scale} already noticed. The adaptation calculation consumes an increasingly amount of time. This can be explained with the increasing model size, since the actual action calculation is optimized for $O(1)$ operations, while the action creation operates with PCM model elements directly. The EMF framework in general shows poor performance characteristics, which can also be seen at the failed 100000 changes evaluation. The adaptation planning needs next to no time.

The standard deviation (\autoref{fig:eval:adap:runtime_sd}) shows mostly constant deviations. However, the adaptation calculation increases significantly on the last evaluation run. We assume this is due to JVM memory management and EMF effects. However, a standard deviation of roughly 3 seconds to a runtime of 30 seconds is within an acceptable ratio, considering that two models and graphs of 30000 nodes have to be managed.

The adaptation runtime in general shows satisfying results. The calculation can be considered very fast with an acceptable variance.





